{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c61cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Documents/Courses/CMU 16-745/Homework/HW1_S24`\n"
     ]
    }
   ],
   "source": [
    "import Pkg\n",
    "Pkg.activate(@__DIR__)\n",
    "Pkg.instantiate()\n",
    "using LinearAlgebra, Plots\n",
    "import ForwardDiff as FD\n",
    "using MeshCat\n",
    "using Test\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6665c60",
   "metadata": {},
   "source": [
    "# Q2: Equality Constrained Optimization (25 pts)\n",
    "In this problem, we are going to use Newton's method to solve some constrained optimization problems. We will start with a smaller problem where we can experiment with Full Newton vs Gauss-Newton, then we will use these methods to solve for the motor torques that make a quadruped balance on one leg."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0500ed",
   "metadata": {},
   "source": [
    "## Part A (10 pts)\n",
    "Here we are going to solve some equality-constrained optimization problems with Newton's method. We are given a problem \n",
    "\n",
    "$$ \\begin{align} \\min_x \\quad & f(x) \\\\ \n",
    " \\mbox{st} \\quad & c(x) = 0 \n",
    " \\end{align}$$\n",
    " Which has the following Lagrangian:\n",
    " $$ \\mathcal{L}(x,\\lambda) = f(x) + \\lambda ^T c(x), $$\n",
    "and the following KKT conditions for optimality:\n",
    "$$\\begin{align}\n",
    "\\nabla_x \\mathcal{L} = \\nabla_x f(x) + \\bigg[ \\frac{\\partial c}{\\partial x}\\bigg] ^T \\lambda &= 0 \\\\ \n",
    "c(x) &= 0 \n",
    "\\end{align}$$\n",
    "\n",
    "Which is just a root-finding problem. To solve this, we are going to solve for a $z = [x^T,\\lambda]^T$ that satisfies these KKT conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f089da1",
   "metadata": {},
   "source": [
    "### Newton's Method with a Linesearch\n",
    "\n",
    "We use Newton's method to solve for when $r(z) = 0$. To do this, we specify `res_fx(z)` as $r(z)$, and `res_jac_fx(z)` as $\\partial r/ \\partial z$. To calculate a Newton step, we do the following:\n",
    "\n",
    "$$\\Delta z = -\\bigg[ \\frac{\\partial r}{\\partial z} \\bigg]^{-1} r(z_k)$$\n",
    "\n",
    "We then decide the step length with a linesearch that finds the largest $\\alpha \\leq 1$ such that the following is true:\n",
    "$$ \\phi(z_k + \\alpha \\Delta z) < \\phi(z_k)$$\n",
    "Where $\\phi$ is a \"merit function\", or `merit_fx(z)` in the code. In this assignment you will use a backtracking linesearch where $\\alpha$ is initialized as $\\alpha = 1.0$, and is divided by 2 until the above condition is satisfied.\n",
    "\n",
    "NOTE: YOU DO NOT NEED TO (AND SHOULD NOT) USE A WHILE LOOP ANYWHERE IN THIS ASSIGNMENT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ef62a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newtons_method (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function linesearch(z::Vector, Δz::Vector, merit_fx::Function;\n",
    "                    max_ls_iters = 10)::Float64 # optional argument with a default\n",
    "    # Return maximum α≤1 such that merit_fx(z + α*Δz) < merit_fx(z)\n",
    "    # with a backtracking linesearch (α = α/2 after each iteration)\n",
    "    α = 1.0\n",
    "    for i = 1:max_ls_iters\n",
    "        # Return α when merit_fx(z + α*Δz) < merit_fx(z)\n",
    "        if merit_fx(z + α * Δz) < merit_fx(z)\n",
    "            return α\n",
    "        end\n",
    "        α /= 2  # Reduce step size\n",
    "    end\n",
    "    error(\"linesearch failed\")\n",
    "end\n",
    "\n",
    "function newtons_method(z0::Vector, res_fx::Function, res_jac_fx::Function, merit_fx::Function;\n",
    "                        tol = 1e-10, max_iters = 50, verbose = false)::Vector{Vector{Float64}}\n",
    "    # Implement Newton's method given the following inputs:\n",
    "    # - z0, initial guess \n",
    "    # - res_fx, residual function \n",
    "    # - res_jac_fx, Jacobian of residual function wrt z \n",
    "    # - merit_fx, merit function for use in linesearch \n",
    "    \n",
    "    # optional arguments \n",
    "    # - tol, tolerance for convergence. Return when norm(residual)<tol \n",
    "    # - max iter, max # of iterations \n",
    "    # - verbose, bool telling the function to output information at each iteration\n",
    "    \n",
    "    # return a vector of vectors containing the iterates \n",
    "    # the last vector in this vector of vectors should be the approx. solution \n",
    "    \n",
    "    Z = [zeros(length(z0)) for i = 1:max_iters]\n",
    "    Z[1] = z0 \n",
    "    \n",
    "    for i = 1:(max_iters - 1)    \n",
    "        # evaluate current residual \n",
    "        norm_r = norm(res_fx(Z[i]))\n",
    "        if verbose \n",
    "            print(\"iter: $i    |r|: $norm_r   \")\n",
    "        end\n",
    "        if norm_r < tol\n",
    "            return Z[1:i]\n",
    "        end\n",
    "        \n",
    "        # caculate Newton step\n",
    "        Δz = - res_jac_fx(Z[i]) \\ res_fx(Z[i])\n",
    "        \n",
    "        # linesearch and update z \n",
    "        α = linesearch(Z[i], Δz, merit_fx)\n",
    "        if verbose\n",
    "            print(\"α: $α \\n\")\n",
    "        end\n",
    "        Z[i+1] = Z[i] + α * Δz        \n",
    "    end\n",
    "    error(\"Newton's method did not converge\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "879bc600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1    |r|: 0.9995239729818045   α: 1.0 \n",
      "iter: 2    |r|: 0.9421342427117169   α: 0.5 \n",
      "iter: 3    |r|: 0.1753172908866053   α: 1.0 \n",
      "iter: 4    |r|: 0.0018472215879181287   α: 1.0 \n",
      "iter: 5    |r|: 2.1010529101114843e-9   α: 1.0 \n",
      "iter: 6    |r|: 2.5246740534795566e-16   \u001b[0m\u001b[1mTest Summary: | \u001b[22m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal  \u001b[22m\u001b[39m\u001b[0m\u001b[1mTime\u001b[22m\n",
      "check Newton  | \u001b[32m   2  \u001b[39m\u001b[36m    2  \u001b[39m\u001b[0m1.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"check Newton\", Any[], 2, false, false, true, 1.73693416332436e9, 1.736934164359626e9, false, \"In[3]\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@testset \"check Newton\" begin \n",
    "    \n",
    "    f(_x) = [sin(_x[1]), cos(_x[2])]\n",
    "    df(_x) = FD.jacobian(f, _x)\n",
    "    merit(_x) = norm(f(_x))\n",
    "    \n",
    "    x0 = [-1.742410372590328, 1.4020334125022704]\n",
    "    \n",
    "    X = newtons_method(x0, f, df, merit; tol = 1e-10, max_iters = 50, verbose = true)\n",
    "    \n",
    "    # check this took the correct number of iterations\n",
    "    # if your linesearch isn't working, this will fail \n",
    "    # you should see 1 iteration where α = 0.5 \n",
    "    @test length(X) == 6 \n",
    "    \n",
    "    # check we actually converged\n",
    "    @test norm(f(X[end])) < 1e-10\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ed4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    function plotting_cost(x::Vector)\n",
    "        Q = [1.65539  2.89376; 2.89376  6.51521];\n",
    "        q = [2;-3]\n",
    "        return 0.5*x'*Q*x + q'*x + exp(-1.3*x[1] + 0.3*x[2]^2)\n",
    "    end\n",
    "    contour(-.6:.1:0,0:.1:.6, (x1,x2)-> plotting_cost([x1;x2]),title = \"Cost Function\",\n",
    "            xlabel = \"X₁\", ylabel = \"X₂\",fill = true)\n",
    "    xcirc = [.5*cos(θ) for θ in range(0, 2*pi, length = 200)]\n",
    "    ycirc = [.5*sin(θ) for θ in range(0, 2*pi, length = 200)]\n",
    "    plot!(xcirc,ycirc, lw = 3.0, xlim = (-.6, 0), ylim = (0, .6),label = \"constraint\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da347c",
   "metadata": {},
   "source": [
    "We will now use Newton's method to solve the following constrained optimization problem. We will write functions for the full Newton Jacobian, as well as the Gauss-Newton Jacobian. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a086487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use Newton's method to solve the constrained optimization problem shown above\n",
    "function cost(x::Vector)\n",
    "    Q = [1.65539  2.89376; 2.89376  6.51521];\n",
    "    q = [2;-3]\n",
    "    return 0.5*x'*Q*x + q'*x + exp(-1.3*x[1] + 0.3*x[2]^2)\n",
    "end\n",
    "function constraint(x::Vector)\n",
    "    norm(x) - 0.5 \n",
    "end\n",
    "# HINT: use this if you want to, but you don't have to\n",
    "function constraint_jacobian(x::Vector)::Matrix\n",
    "    # since `constraint` returns a scalar value, ForwardDiff \n",
    "    # will only allow us to compute a gradient of this function \n",
    "    # (instead of a Jacobian). This means we have two options for\n",
    "    # computing the Jacobian: Option 1 is to just reshape the gradient\n",
    "    # into a row vector\n",
    "    \n",
    "    # J = reshape(FD.gradient(constraint, x), 1, 2)\n",
    "    \n",
    "    # or we can just make the output of constraint an array, \n",
    "    constraint_array(_x) = [constraint(_x)]\n",
    "    J = FD.jacobian(constraint_array, x)\n",
    "    \n",
    "    # assert the jacobian has # rows = # outputs \n",
    "    # and # columns = # inputs \n",
    "    @assert size(J) == (length(constraint(x)), length(x))\n",
    "    \n",
    "    return J \n",
    "end\n",
    "\n",
    "function kkt_conditions(z::Vector)::Vector\n",
    "    # TODO: return the KKT conditions\n",
    "\n",
    "    x = z[1:2]\n",
    "    λ = z[3:3]\n",
    "\n",
    "    # return the stationarity condition for the cost function\n",
    "    # and the primal feasibility\n",
    "    startionarity_condition = FD.gradient(cost, x) .+ λ .* FD.gradient(constraint, x)\n",
    "    primal_feasibility = constraint(x)\n",
    "    \n",
    "    # error(\"kkt not implemented\")\n",
    "    return vcat(startionarity_condition, primal_feasibility)\n",
    "end\n",
    "\n",
    "function fn_kkt_jac(z::Vector)::Matrix\n",
    "    # error(\"fn_kkt_jac not implemented\")\n",
    "    \n",
    "    # return full Newton Jacobian of kkt conditions wrt z\n",
    "    x = z[1:2]\n",
    "    λ = z[3]\n",
    "\n",
    "    # return full Newton jacobian with a 1e-3 regularizer\n",
    "    hessian_cost = FD.hessian(cost, x)\n",
    "    hessian_constraint = FD.hessian(constraint, x)\n",
    "    jacobian_x_x = hessian_cost + λ * hessian_constraint + 1e-3 * I\n",
    "    jacobian_x_lambda = FD.gradient(constraint, x)\n",
    "    jacobian_constraint_x = FD.gradient(constraint, x)'\n",
    "    jacobian_constraint_lambda = -1e-3 * ones(1, 1)\n",
    "    \n",
    "    # Assemble full Jacobian\n",
    "    return vcat(\n",
    "        hcat(jacobian_x_x, jacobian_x_lambda),          # Stationarity condition\n",
    "        hcat(jacobian_constraint_x, jacobian_constraint_lambda)  # Primal feasibility\n",
    "    ) \n",
    "end\n",
    "\n",
    "# function fn_kkt_jac(z::Vector)::Matrix\n",
    "#     J = FD.jacobian(kkt_conditions, z)\n",
    "#     J[1:2, 1:2] += 1e-3 * I\n",
    "#     J[3, 3] -= 1e-3\n",
    "#     return J\n",
    "# end\n",
    "\n",
    "function gn_kkt_jac(z::Vector)::Matrix\n",
    "    # error(\"gn_kkt_jac not implemented\")\n",
    "    # TODO: return Gauss-Newton Jacobian of kkt conditions wrt z \n",
    "    x = z[1:2]\n",
    "    λ = z[3]\n",
    "\n",
    "    # TODO: return Gauss-Newton jacobian with a 1e-3 regularizer\n",
    "    hessian_cost = FD.hessian(cost, x)\n",
    "    jacobian_x_x = hessian_cost + 1e-3 * I\n",
    "    jacobian_x_lambda = FD.gradient(constraint, x)\n",
    "    jacobian_constraint_x = FD.gradient(constraint, x)'\n",
    "    jacobian_constraint_lambda = -1e-3 * ones(1, 1)\n",
    "    \n",
    "    # Assemble full Jacobian\n",
    "    return vcat(\n",
    "        hcat(jacobian_x_x, jacobian_x_lambda),          # Stationarity condition\n",
    "        hcat(jacobian_constraint_x, jacobian_constraint_lambda)  # Primal feasibility\n",
    "    ) \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9fb793",
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"Test Jacobians\" begin \n",
    "    \n",
    "    # first we check the regularizer \n",
    "    z = randn(3)\n",
    "    J_fn = fn_kkt_jac(z)\n",
    "    J_gn = gn_kkt_jac(z)\n",
    "    \n",
    "    # check what should/shouldn't be the same between \n",
    "    @test norm(J_fn[1:2,1:2] - J_gn[1:2,1:2]) > 1e-10\n",
    "    @test abs(J_fn[3,3] + 1e-3) < 1e-10\n",
    "    @test abs(J_gn[3,3] + 1e-3) < 1e-10\n",
    "    @test norm(J_fn[1:2,3] - J_gn[1:2,3]) < 1e-10\n",
    "    @test norm(J_fn[3,1:2] - J_gn[3,1:2]) < 1e-10\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46b09aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"Full Newton\" begin \n",
    "    \n",
    "    \n",
    "    z0 = [-.1, .5, 0] # initial guess\n",
    "    merit_fx(_z) = norm(kkt_conditions(_z)) # simple merit function\n",
    "    Z = newtons_method(z0, kkt_conditions, fn_kkt_jac, merit_fx; tol = 1e-4, max_iters = 100, verbose = true)\n",
    "    R = kkt_conditions.(Z)\n",
    "\n",
    "    # make sure we converged on a solution to the KKT conditions \n",
    "    @test norm(kkt_conditions(Z[end])) < 1e-4\n",
    "    @test length(R) < 6\n",
    "    \n",
    "    \n",
    "    # ------------------------plotting stuff------------------------\n",
    "    Rp = [[abs(R[i][ii]) + 1e-15 for i = 1:length(R)] for ii = 1:length(R[1])] # this gets abs of each term at each iteration\n",
    "    \n",
    "    plot(Rp[1],yaxis=:log,ylabel = \"|r|\",xlabel = \"iteration\",\n",
    "         yticks= [1.0*10.0^(-x) for x = float(15:-1:-2)],\n",
    "         title = \"Convergence of Full Newton on KKT Conditions\",label = \"|r_1|\")\n",
    "    plot!(Rp[2],label = \"|r_2|\")\n",
    "    display(plot!(Rp[3],label = \"|r_3|\"))\n",
    "    \n",
    "    contour(-.6:.1:0,0:.1:.6, (x1,x2)-> cost([x1;x2]),title = \"Cost Function\",\n",
    "            xlabel = \"X₁\", ylabel = \"X₂\",fill = true)\n",
    "    xcirc = [.5*cos(θ) for θ in range(0, 2*pi, length = 200)]\n",
    "    ycirc = [.5*sin(θ) for θ in range(0, 2*pi, length = 200)]\n",
    "    plot!(xcirc,ycirc, lw = 3.0, xlim = (-.6, 0), ylim = (0, .6),label = \"constraint\")\n",
    "    z1_hist = [z[1] for z in Z]\n",
    "    z2_hist = [z[2] for z in Z]\n",
    "    display(plot!(z1_hist, z2_hist, marker = :d, label = \"xₖ\"))\n",
    "    # ------------------------plotting stuff------------------------\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"Gauss-Newton\" begin \n",
    "    \n",
    "    \n",
    "    z0 = [-.1, .5, 0] # initial guess\n",
    "    merit_fx(_z) = norm(kkt_conditions(_z)) # simple merit function\n",
    "    \n",
    "    # the only difference in this block vs the previous is `gn_kkt_jac` instead of `fn_kkt_jac`\n",
    "    Z = newtons_method(z0, kkt_conditions, gn_kkt_jac, merit_fx; tol = 1e-4, max_iters = 100, verbose = true)\n",
    "    R = kkt_conditions.(Z)\n",
    "\n",
    "    # make sure we converged on a solution to the KKT conditions \n",
    "    @test norm(kkt_conditions(Z[end])) < 1e-4\n",
    "    @test length(R) < 10\n",
    "    \n",
    "    \n",
    "    # ------------------------plotting stuff------------------------\n",
    "    Rp = [[abs(R[i][ii]) + 1e-15 for i = 1:length(R)] for ii = 1:length(R[1])] # this gets abs of each term at each iteration\n",
    "    \n",
    "    plot(Rp[1],yaxis=:log,ylabel = \"|r|\",xlabel = \"iteration\",\n",
    "         yticks= [1.0*10.0^(-x) for x = float(15:-1:-2)],\n",
    "         title = \"Convergence of Full Newton on KKT Conditions\",label = \"|r_1|\")\n",
    "    plot!(Rp[2],label = \"|r_2|\")\n",
    "    display(plot!(Rp[3],label = \"|r_3|\"))\n",
    "    \n",
    "    contour(-.6:.1:0,0:.1:.6, (x1,x2)-> cost([x1;x2]),title = \"Cost Function\",\n",
    "            xlabel = \"X₁\", ylabel = \"X₂\",fill = true)\n",
    "    xcirc = [.5*cos(θ) for θ in range(0, 2*pi, length = 200)]\n",
    "    ycirc = [.5*sin(θ) for θ in range(0, 2*pi, length = 200)]\n",
    "    plot!(xcirc,ycirc, lw = 3.0, xlim = (-.6, 0), ylim = (0, .6),label = \"constraint\")\n",
    "    z1_hist = [z[1] for z in Z]\n",
    "    z2_hist = [z[2] for z in Z]\n",
    "    display(plot!(z1_hist, z2_hist, marker = :d, label = \"xₖ\"))\n",
    "    # ------------------------plotting stuff------------------------\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e5747",
   "metadata": {},
   "source": [
    "## Part B (10 pts): Balance a quadruped\n",
    "Now we are going to solve for the control input $u \\in \\mathbb{R}^{12}$, and state $x \\in \\mathbb{R}^{30}$, such that the quadruped is balancing up on one leg at an equilibrium point. First, let's load in a dynamics model from `quadruped.jl`, where \n",
    "\n",
    "$ \\dot{x} = f(x,u) =$ `dynamics(model, x, u)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95fb0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include the functions from quadruped.jl\n",
    "include(joinpath(@__DIR__, \"quadruped.jl\"))\n",
    "\n",
    "# this loads in our continuous time dynamics function xdot = dynamics(model, x, u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c76499",
   "metadata": {},
   "source": [
    "let's load in a model and display the rough \"guess\" configuration that we are going for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50ea3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------these three are global variables------------\n",
    "model = UnitreeA1() # contains all the model properties for the quadruped\n",
    "mvis = initialize_visualizer(model) # visualizer \n",
    "const x_guess = initial_state(model) # our guess state for balancing\n",
    "# ----------------------------------------------------\n",
    "\n",
    "set_configuration!(mvis, x_guess[1:state_dim(model)÷2])\n",
    "render(mvis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5826be3",
   "metadata": {},
   "source": [
    "Now, we are going to solve for the state and control that get us an equilibrium (balancing) on just one leg. We are going to do this by solving the following optimization problem:\n",
    "\n",
    "$$ \\begin{align} \\min_{x,u} \\quad & \\frac{1}{2}(x - x_{guess})^T(x - x_{guess}) + \\frac{1}{2}10^{-3} u^Tu \\\\ \n",
    " \\mbox{st} \\quad & \\dot{x} = f(x,u) = 0 \n",
    " \\end{align}$$\n",
    " \n",
    " Where our primal variables are $x \\in \\mathbb{R}^{30}$ and $u \\in \\mathbb{R}^{12}$, that we can stack up in a new variable $y = [x^T, u^T]^T \\in \\mathbb{R}^{42}$. We have a constraint $\\dot{x} = f(x,u) = 0$, which will ensure the resulting configuration is an equilibrium. This constraint is enforced with a dual variable $\\lambda \\in \\mathbb{R}^{30}$. We are now ready to use Newton's method to solve this equality constrained optimization problem, where we will solve for a variable $z = [y^T, \\lambda^T]^T \\in \\mathbb{R}^{72}$.\n",
    " \n",
    " In this next section, you should fill out `quadruped_kkt(z)` with the KKT conditions for this optimization problem, given the constraint is that `dynamics(model, x, u) = zeros(30)`. When forming the Jacobian of the KKT conditions, use the Gauss-Newton approximation for the hessian of the Lagrangian (see example above if you're having trouble with this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6bfbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial guess \n",
    "const x_guess = initial_state(model)\n",
    "\n",
    "# indexing stuff \n",
    "const idx_x = 1:30 \n",
    "const idx_u = 31:42\n",
    "const idx_c = 43:72\n",
    "\n",
    "function quadruped_cost(y::Vector)\n",
    "    # cost function \n",
    "    @assert length(y) == 42\n",
    "    x = y[idx_x]\n",
    "    u = y[idx_u]\n",
    "    \n",
    "    # error(\"quadruped cost not implemented\")\n",
    "    cost = 0.5 * (x - x_guess)' * (x - x_guess) + 0.5 * 1e-3 * u' * u\n",
    "    return cost\n",
    "end\n",
    "\n",
    "function quadruped_constraint(y::Vector)::Vector\n",
    "    # constraint function \n",
    "    @assert length(y) == 42\n",
    "    x = y[idx_x]\n",
    "    u = y[idx_u]\n",
    "    \n",
    "    return dynamics(model, x, u)\n",
    "end\n",
    "\n",
    "function quadruped_kkt(z::Vector)::Vector\n",
    "    # Return the KKT conditions \n",
    "    @assert length(z) == 72 \n",
    "    x = z[idx_x]\n",
    "    u = z[idx_u]\n",
    "    λ = z[idx_c]\n",
    "    \n",
    "    y = [x;u]\n",
    "    \n",
    "    # Compute the stationarity condition\n",
    "    grad_cost = ForwardDiff.gradient(quadruped_cost, y)  # (42,)\n",
    "    jacobian_constraint = ForwardDiff.jacobian(quadruped_constraint, y)  # (30, 42)\n",
    "    stationarity_condition = grad_cost .+ jacobian_constraint' * λ  # (42,)\n",
    "\n",
    "    # Compute the primal feasibility\n",
    "    primal_feasibility = quadruped_constraint(y)  # (30,)\n",
    "\n",
    "    # Concatenate the results\n",
    "    return vcat(stationarity_condition, primal_feasibility)  # (72,)  \n",
    "end\n",
    "\n",
    "function quadruped_kkt_jac(z::Vector)::Matrix\n",
    "    @assert length(z) == 72 \n",
    "    x = z[idx_x]\n",
    "    u = z[idx_u]\n",
    "    λ = z[idx_c]\n",
    "    \n",
    "    y = [x;u]\n",
    "    \n",
    "    # Return Gauss-Newton Jacobian with a regularizer (try 1e-3,1e-4,1e-5,1e-6)\n",
    "    regularizer = 1e-3\n",
    "    hessian_cost = ForwardDiff.hessian(quadruped_cost, y)  # (42, 42)\n",
    "    jacobian_y_lambda = ForwardDiff.jacobian(quadruped_constraint, y)  # (30, 42)\n",
    "    \n",
    "    # Add regularizer to the diagonal of Hessian\n",
    "    jacobian_y_y = hessian_cost + regularizer * I(42)\n",
    "    \n",
    "    # Assemble full Jacobian\n",
    "    return vcat(\n",
    "        hcat(jacobian_y_y, jacobian_y_lambda'),          # Stationarity condition (42 rows)\n",
    "        hcat(jacobian_y_lambda, -regularizer * I(30))           # Primal feasibility (30 rows)\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa84cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "function quadruped_merit(z)\n",
    "    # merit function for the quadruped problem \n",
    "    @assert length(z) == 72 \n",
    "    r = quadruped_kkt(z)\n",
    "    return norm(r[1:42]) + 1e4*norm(r[43:end])\n",
    "end\n",
    "\n",
    "@testset \"quadruped standing\" begin\n",
    "    \n",
    "    z0 = [x_guess; zeros(12); zeros(30)]\n",
    "    Z = newtons_method(z0, quadruped_kkt, quadruped_kkt_jac, quadruped_merit; tol = 1e-6, verbose = true, max_iters = 50)\n",
    "    set_configuration!(mvis, Z[end][1:state_dim(model)÷2])\n",
    "    R = norm.(quadruped_kkt.(Z))\n",
    "    \n",
    "    display(plot(1:length(R), R, yaxis=:log,xlabel = \"iteration\", ylabel = \"|r|\"))\n",
    "    \n",
    "    @test R[end] < 1e-6\n",
    "    @test length(Z) < 25\n",
    "    \n",
    "    x,u = Z[end][idx_x], Z[end][idx_u]\n",
    "    \n",
    "    @test norm(dynamics(model, x, u)) < 1e-6\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a570f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    \n",
    "    # let's visualize the balancing position we found\n",
    "    \n",
    "    z0 = [x_guess; zeros(12); zeros(30)]\n",
    "    Z = newtons_method(z0, quadruped_kkt, quadruped_kkt_jac, quadruped_merit; tol = 1e-6, verbose = false, max_iters = 50)\n",
    "    # visualizer \n",
    "    mvis = initialize_visualizer(model)\n",
    "    set_configuration!(mvis, Z[end][1:state_dim(model)÷2])\n",
    "    render(mvis)\n",
    "    \n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc85af",
   "metadata": {},
   "source": [
    "## Part C (5 pts): One sentence short answer\n",
    "\n",
    "1. Why do we use a linesearch? \n",
    "\n",
    "**To ensure the object is always decreasing.**\n",
    "\n",
    "2. Do we need a linesearch for both convex and nonconvex problems?\n",
    "\n",
    "**Yes, we do.**\n",
    "\n",
    "3. Name one case where we absolutely do not need a linesearch.\n",
    "\n",
    "**We are solving a quadratic problem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9432b668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
